{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 5 - Vision et language\n",
    "\n",
    "**Dans ce TP, nous abordons le problème du légendage d’images (« image captioning »), qui consiste à décrire le contenu visuel d’une image par une phrase en language naturel. Nous allons mettre en place une version simplifié de l’approche « show and tell » [[VTBE15]](#vinyalstbecvpr2015).**\n",
    "\n",
    "Le modèle va analyser une image en entrée, et à partir d’un symbole de début de séquence (\"\\< start\\>\"), va apprendre à générer le mot suivant de la légende. \n",
    "D’une manière générale, à partit d’un sous-ensemble de mot de la phrase généré et l’image d’entrée, l’objectif va être d’apprendre au modèle à générer le mot suivant, jusqu’à arriver au symbole de fin de génération (“\\< end\\>”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 : Simplification du vocabulaire\n",
    "\n",
    "Pour accélérer le temps nécessaire à l’entraînement du modèle, nous allons considérer un sous-ensemble du vocabulaire de mots considéré au TP précédent (voir [Exercice 2 : Embedding Vectoriel de texte](http://cedric.cnam.fr/~thomen/cours/US330X/tpRNNs.html#tp4-embedding)). On pourra utiliser le code suivant pour extraire un histogramme d’occurrence des mots présents dans les légendes du sous-ensemble d’apprentissage de la base Flickr8k :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get  # to make GET request\n",
    "\n",
    "def download(url, file_name):\n",
    "    # open in binary mode\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        # get request\n",
    "        response = get(url)\n",
    "        # write to file\n",
    "        file.write(response.content)\n",
    "\n",
    "download('http://cedric.cnam.fr/~thomen/cours/US330X/flickr_8k_train_dataset.txt',\"flickr_8k_train_dataset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import _pickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "filename = 'flickr_8k_train_dataset.txt'\n",
    "df = pd.read_csv(filename, delimiter='\\t')\n",
    "nb_samples = df.shape[0]\n",
    "iter = df.iterrows()\n",
    "\n",
    "bow = {}\n",
    "nbwords = 0\n",
    "\n",
    "for i in range(nb_samples):\n",
    "    x = iter.__next__()\n",
    "    cap_words = x[1][1].split() # split caption into words\n",
    "    cap_wordsl = [w.lower() for w in cap_words] # remove capital letters\n",
    "    nbwords += len(cap_wordsl)\n",
    "    for w in cap_wordsl:\n",
    "        if (w in bow):\n",
    "            bow[w] = bow[w]+1\n",
    "        else:\n",
    "            bow[w] = 1\n",
    "\n",
    "bown = sorted([(value,key) for (key,value) in bow.items()], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On pourra calculer la fréquence cumulée des 1000 premiers mots conservés :** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbkeep = 1000 \n",
    "freqnc = np.cumsum([float(w[0])/nbwords*100.0 for w in bown])\n",
    "print(\"number of kept words=\"+str(nbkeep)+\" - ratio=\"+str(freqnc[nbkeep-1])+\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Et affichier la liste des 100 premiers mots :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "x_axis = [str(bown[i][1]) for i in range(100)]\n",
    "plt.figure(dpi=300)\n",
    "plt.xticks(rotation=90, fontsize=3)\n",
    "plt.ylabel('Word Frequency')\n",
    "plt.bar(x_axis, freqnc[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On va charger le fichier d’embedding utilisé au TP précédent (que l’on peut récupérer ici : [http://cedric.cnam.fr/~thomen/cours/US330X/Caption_Embeddings.p](http://cedric.cnam.fr/~thomen/cours/US330X/Caption_Embeddings.p)), et conserver les `nbkeep` (=1000) mots les plus fréquents, et sauvegarder le sous-ensemble de mots et les embeddings vectoriels Glove correspondants :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "download('http://cedric.cnam.fr/~thomen/cours/US330X/Caption_Embeddings.p',\"Caption_Embeddings.p\")\n",
    "outfile = 'Caption_Embeddings.p'\n",
    "[listwords, embeddings] = pickle.load( open( outfile, \"rb\" ) )\n",
    "\n",
    "embeddings_new = np.zeros((nbkeep,102))\n",
    "listwords_new = []\n",
    "\n",
    "for i in range(nbkeep):\n",
    "    listwords_new.append(bown[i][1])\n",
    "    embeddings_new[i,:] = # COMPLETE WITH YOUR CODE\n",
    "    embeddings_new[i,:] /= np.linalg.norm(embeddings_new[i,:]) # Normalization\n",
    "\n",
    "listwords = listwords_new\n",
    "embeddings = embeddings_new\n",
    "outfile = \"Caption_Embeddings_\"+str(nbkeep)+\".p\"\n",
    "\n",
    "with open(outfile, \"wb\" ) as pickle_f:\n",
    "    pickle.dump( [listwords, embeddings], pickle_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 : Création des données d’apprentissage et de test\n",
    "\n",
    "**Nous allons maintenant stocker les données d’entraînement, *i.e.* les tenseurs contenant les données et les labels.**\n",
    "Le tenseur des données $ \\mathbf{X} $ sera de taille $ N_s \\times L_s \\times d $ où $ N_s $ est le nombre de séquence (légendes), $ L_s $ est la longueur de la séquence et  $ d $ est la taille du vecteur décrivant chaque mot de la séquence.\n",
    "\n",
    "Les fichiers sources contenant les identifiants des images et les légendes des images sont disponibles :\n",
    "\n",
    "- Ici pour l’apprentissage : [http://cedric.cnam.fr/~thomen/cours/US330X/flickr_8k_train_dataset.txt](http://cedric.cnam.fr/~thomen/cours/US330X/flickr_8k_train_dataset.txt)  \n",
    "- Ici pour le test : [http://cedric.cnam.fr/~thomen/cours/US330X/flickr_8k_test_dataset.txt](http://cedric.cnam.fr/~thomen/cours/US330X/flickr_8k_test_dataset.txt)  \n",
    "\n",
    "\n",
    "Comme on doit utiliser des tenseurs de taille fixe avec `keras`, on va déterminer la longueur de la légende maximale dans les données d’entraînement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "filename = 'flickr_8k_train_dataset.txt'\n",
    "df = pd.read_csv(filename, delimiter='\\t')\n",
    "nbTrain = df.shape[0]\n",
    "iter = df.iterrows()\n",
    "\n",
    "caps = [] # Set of captions\n",
    "imgs = [] # Set of images\n",
    "for i in range(nbTrain):\n",
    "    x = iter.__next__()\n",
    "    caps.append(x[1][1])\n",
    "    imgs.append(x[1][0])\n",
    "\n",
    "maxLCap = 0\n",
    "\n",
    "for caption in caps:\n",
    "    l=0\n",
    "    words_in_caption =  caption.split()\n",
    "    for j in range(len(words_in_caption)-1):\n",
    "        current_w = words_in_caption[j].lower()\n",
    "        if(current_w in listwords):\n",
    "            l+=1    \n",
    "        if(l > maxLCap):\n",
    "            maxLCap = l\n",
    "\n",
    "print(\"max caption length =\"+str(maxLCap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chaque élément $ e_i $ d’une séquence d’entrée va être décrit par par un vecteur $ x_i \\in \\mathbb{R}^d $ (avec ici $ d=202 $) correspondant à :**\n",
    "\n",
    "- La description du contenu du $ i^{\\grave{e}me} $ mot de la légende, pour laquelle on utilisera l’embedding vectoriel Glove (voir TP précédent [Exercice 2 : Embedding Vectoriel de texte](http://cedric.cnam.fr/~thomen/cours/US330X/tpRNNs.html#tp4-embedding)). Ceci correspondra aux 102 premières composantes de $ x_i $.  \n",
    "- La description du contenu visuel de l’image, obtenue par le calcul de « Deep Features » (voir [TP 3 - Transfer Learning et Fine-Tuning](http://cedric.cnam.fr/~thomen/cours/US330X/tpTransfer.html#chap-tptransfer)), ici en  utilisant un réseau convolutif profond  de type VGG [[SZ15]](#dblp-journals-corr-simonyanz14a). La dimension du vecteur résultant (ici 4096) a été réduite à 100 par Analyse en Composantes Principales (ACP). Ceci correspondra aux 100 dernières composantes de $ x_i $. **Ce vecteur de dimension 100 pour chaque image de la base est fourni :** [http://cedric.cnam.fr/~thomen/cours/US330X/encoded_images_PCA.p](http://cedric.cnam.fr/~thomen/cours/US330X/encoded_images_PCA.p)  \n",
    "\n",
    "\n",
    "**La sortie du réseau récurrent est une séquence de la même taille que l’entrée, où chaque élément correspond à la prédiction du mot suivant.** Chaque séquence de sortie se terminera donc toujours par “<end>”. Le vocabulaire ayant été simplifié à l’exercice 1, on ne conservera dans les séquences d’entrée et de sortie que les mots de la légende présents dans le dictionnaire réduit.\n",
    "\n",
    "**On demande de compléter le code suivant pour construire les tenseurs des données et des labels :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "indexwords = {}\n",
    "for i in range(len(listwords)):\n",
    "    indexwords[listwords[i]] = i\n",
    "        \n",
    "tinput = 202\n",
    "\n",
    "# IGNORE START => we will never predict <start> . \n",
    "tVocabulary = len(listwords) \n",
    "\n",
    "X_train = np.zeros((nbTrain,maxLCap, tinput))\n",
    "Y_train = np.zeros((nbTrain,maxLCap, tVocabulary), bool)\n",
    "\n",
    "ll = 50\n",
    "#binmat = np.zeros((ll,39))\n",
    "nbtot = 0 \n",
    "nbkept=0\n",
    "       \n",
    "for i in range(nbTrain):\n",
    "    words_in_caption =  caps[i].split() \n",
    "  \n",
    "    nbtot += len(words_in_caption)-1\n",
    "    indseq=0\n",
    "    for j in range(len(words_in_caption)-1):\n",
    "        \n",
    "        current_w = words_in_caption[j].lower()\n",
    "        \n",
    "        if(j==0 and current_w!='<start>'): \n",
    "            print(\"PROBLEM\")\n",
    "        if(current_w in listwords): \n",
    "            X_train[i,indseq,0:100] = # COMPLETE WITH YOUR CODE\n",
    "            X_train[i,indseq,100:202] = # COMPLETE WITH YOUR CODE\n",
    "            \n",
    "        next_w = words_in_caption[j+1].lower()\n",
    "        \n",
    "        #print(\"current_w=\"+str(current_w)+\" next_w=\"+str(next_w)+\" indseq=\"+str(indseq))\n",
    "        index_pred = 0\n",
    "        if(next_w in listwords): \n",
    "            nbkept +=1\n",
    "            index_pred = # COMPLETE WITH YOUR CODE\n",
    "            Y_train[i,indseq,index_pred] = # COMPLETE WITH YOUR CODE\n",
    "        \n",
    "            indseq += 1\n",
    "            \n",
    "outfile = 'Training_data_'+str(nbkeep)\n",
    "np.savez(outfile, X_train=X_train, Y_train=Y_train) # Saving tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On fera la même chose pour les données de test, en allouant des tenseurs de la même taille que ceux d’entraînement (et pouvoir ainsi appliquer le modèle de prédiction ensuite).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download('http://cedric.cnam.fr/~thomen/cours/US330X/flickr_8k_test_dataset.txt',\"flickr_8k_test_dataset.txt\")\n",
    "filenameTest = 'flickr_8k_test_dataset.txt'\n",
    "# TODO : \n",
    "# - Parsing des images et légendes de test\n",
    "# - Création des tenseurs des données et labels de test\n",
    "# - Remplir les tenseurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3 : Entraînement du modèle\n",
    "\n",
    "On va maintenant définir l’architecture du modèle pour apprendre à prédire le mot suivant à partir d’une sous-séquence donnée et d’une image. Après avoir instancié un modèle `Sequential` vide, on va donc définir :\n",
    "\n",
    "- Une couche de `Masking`, voir la documentation [https://keras.io/layers/core/#masking](https://keras.io/layers/core/#masking). On doit préciser la taille d’entrée de chaque exemple (séquence), *i.e.* $ L_s \\times d $ ($ L_s $ longueur de la séquence et $ d $ taille de chaque élément de la séquence).  \n",
    "\n",
    "\n",
    "> - Cette couche de `Masking` va permettre de ne pas calculer d’erreur dans les zones où le tenseur d’entrée sera à une valeur donné (*e.g.* 0), ce qui correspondra au zones de la séquence d’entrée où aucun mot n’est présent (du fait de la nécessité d’avoir un tenseur de taille fixe, donc une longeur de séquence correspondant à la séquence de longueur maximale dans la base d’apprentissage).  \n",
    "\n",
    "\n",
    "\n",
    "- Une couche de réseau récurrent de type `SimpleRNN`. On prendra 100 neurones dans la couche cachée. On utiliser `return_sequences=True` pour renvoyer la prédiction pour chaque élément de la séquence d’entrée,  et également `unroll=True`.  \n",
    "- Une couche complètement connectée, suivie d’une fonction d’activation softmax.  \n",
    "\n",
    "\n",
    "Pour apprendre le modèle, on va définir un fonction de coût d’entropie croisée, qui va être directement appliqué à chacun des élément de sortie. On utilisera l’optimiseur `Adam`, avec le pas de gradient par défaut. On choisira une taille de batch de 10 exemples (*i.e.* 10 légendes, l’erreur de classification sur chaque légende étant calculée comme une moyenne des prédictions sur chaque mot).\n",
    "\n",
    "- Une fois le modèle entraîné on le sauvera avec la fonction habituelle :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_yaml\n",
    "\n",
    "def saveModel(model, savename):\n",
    "    # serialize model to YAML\n",
    "    model_yaml = model.to_yaml()\n",
    "    with open(savename+\".yaml\", \"w\") as yaml_file:\n",
    "        yaml_file.write(model_yaml)\n",
    "    print(\"Yaml Model \",savename,\".yaml saved to disk\")    \n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(savename+\".h5\")\n",
    "    print(\"Weights \",savename,\".h5 saved to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On évaluera les performances sur la base d’apprentissage avec la méthode `evaluate()`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 4 : Évaluation du modèle\n",
    "\n",
    "**Finalement, on va utiliser le réseau récurrent pour générer une légende sur une image de test, et analyser qualitativement le résultat.** On peut télécharger la base d’images ici : [http://cedric.cnam.fr/~thomen/cours/US330X/Flickr8k_Dataset.zip](http://cedric.cnam.fr/~thomen/cours/US330X/Flickr8k_Dataset.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download('http://cedric.cnam.fr/~thomen/cours/US330X/Flickr8k_Dataset.zip',\"Flickr8k_Dataset.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf Flickr8k_Dataset.zip # !unzip Flickr8k_Dataset.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B. : en test, on dispose uniquement d’une image, et le système va, à partir du symbole “\\< start>”, itérativement produire une séquence de mots jusqu’à générer le symbole de fin de séquence”\\< end>”**\n",
    "\n",
    "On va commencer par charger un modèle appris, les données de test et les embeddings vectoriels avec le dictionnaire réduit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def loadModel(savename):\n",
    "    #print \"Loading Yam model \",saveName\n",
    "    with open(savename+\".yaml\", \"r\") as yaml_file:\n",
    "        model = model_from_yaml(yaml_file.read()) \n",
    "    print(\"Yaml Model \",savename,\".yaml loaded \")   \n",
    "    model.load_weights(savename+\".h5\")\n",
    "    print(\"Weights \",savename,\".h5 loaded \")\n",
    "    return model\n",
    "\n",
    "# LOADING MODEL\n",
    "nameModel =  # COMPLETE with your model name\n",
    "model = loadModel(nameModel)\n",
    "\n",
    "optim = Adam()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optim,metrics=['accuracy'])\n",
    "\n",
    "# LOADING TEST DATA\n",
    "outfile = 'Testing_data_'+str(nbkeep)+'.npz'\n",
    "npzfile = np.load(outfile)\n",
    "\n",
    "X_test = npzfile['X_test']\n",
    "Y_test = npzfile['Y_test']\n",
    "\n",
    "outfile = \"Caption_Embeddings_\"+str(nbkeep)+\".p\"\n",
    "[listwords, embeddings] = pickle.load( open( outfile, \"rb\" ) )\n",
    "indexwords = {}\n",
    "for i in range(len(listwords)):\n",
    "    indexwords[listwords[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On va ensuite sélectionner une image parmi l’ensemble de test, l’afficher, ainsi qu’une légende issues des annotations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ind = np.random.randint(X_test.shape[0])\n",
    "\n",
    "filename = 'flickr_8k_test_dataset.txt' #  PATH IF NEEDED\n",
    "\n",
    "df = pd.read_csv(filename, delimiter='\\t')\n",
    "iter = df.iterrows()\n",
    "\n",
    "for i in range(ind+1):\n",
    "    x = iter.__next__()\n",
    "\n",
    "imname = x[1][0]\n",
    "print(\"image name=\"+imname+\" caption=\"+x[1][1])\n",
    "dirIm = \"data/flickr8k/Flicker8k_Dataset/\" # CHANGE WITH YOUR DATASET\n",
    "\n",
    "img=mpimg.imread(dirIm+imname)\n",
    "plt.figure(dpi=100)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour effecteur la prédiction, on va partir du premier élément de la séquence (*i.e.* contenant l’image et le symbole “<start>”), et effectuer la prédiction :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_test[ind:ind+1,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On va ensuite pouvoir effectuer plusieurs générations de légendes, en échantillonnant le mot de suivant à partir de la distribution *a posteriori* issue du softmax**, comme ceci a été fait dans la partie [c) Génération de texte avec le modèle appris](tpRNNs.ipynb#text-gen) du [TP 4 - Réseaux de neurones récurrents](http://cedric.cnam.fr/~thomen/cours/US330X/tpRNNs.html) (voir la fonction `sampling`). Une fois le mot suivant échantillonné, on place son embedding vectoriel comme entrée pour l’élément suivant de la séquence, et la prédiction continue (jusqu’à arriver au symbole de fin de séquence <”end”>).\n",
    "On pourra donc compléter le code suivant pour générer les légendes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "nbGen = 5\n",
    "temperature=0.1 # Temperature param for peacking soft-max distribution\n",
    "\n",
    "for s in range(nbGen):\n",
    "    wordpreds = \"Caption n° \"+str(s+1)+\": \"\n",
    "    indpred = sampling(pred[0,0,:], temperature)\n",
    "    wordpred = listwords[indpred] \n",
    "    wordpreds +=str(wordpred)+ \" \"\n",
    "    X_test[ind:ind+1,1,100:202] = embeddings[indpred]\n",
    "    cpt=1\n",
    "    while(str(wordpred)!='<end>' and cpt<30):\n",
    "        pred = model.predict(X_test[ind:ind+1,:,:])\n",
    "        indpred = sampling(pred[0,cpt,:], temperature)\n",
    "        wordpred = listwords[indpred] \n",
    "        wordpreds += str(wordpred)+ \" \"\n",
    "        cpt+=1\n",
    "        X_test[ind:ind+1,cpt,100:202] = embeddings[indpred]\n",
    "        \n",
    "    print(wordpreds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultats :\n",
    "\n",
    "**Présentez des résultas de légendage sur différentes images.** Vous pouvez tester avec un vocabulaire simplifié à 100 mots puis à 1000 mots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enfin, on va pouvoir calculer la métrique de blue score après avoir déterminé les prédictions de tous les exemples.** On pourra se baser sur le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.models import model_from_yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# LOADING TEST DATA\n",
    "nbkeep = 1000\n",
    "outfile = \"path\" # REPLACE WITH YOUR DATA PATH\n",
    "outfile += 'Testing_data_'+str(nbkeep)+'.npz'\n",
    "npzfile = np.load(outfile)\n",
    "\n",
    "X_test = npzfile['X_test']\n",
    "Y_test = npzfile['Y_test']\n",
    "\n",
    "# LOADING MODEL\n",
    "nameModel = \"\" REPLACE WITH YOUR MODEL NAME\n",
    "model = loadModel(nameModel)\n",
    "\n",
    "# COMPILING MODEL\n",
    "optim = Adam()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optim,metrics=['accuracy'])\n",
    "scores_test = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"PERFS TEST: %s: %.2f%%\" % (model.metrics_names[1], scores_test[1]*100))\n",
    "\n",
    "# LOADING TEXT EMBEDDINGS\n",
    "outfile = \"Caption_Embeddings_\"+str(nbkeep)+\".p\"\n",
    "[listwords, embeddings] = pickle.load( open( outfile, \"rb\" ) )\n",
    "indexwords = {}\n",
    "for i in range(len(listwords)):\n",
    "    indexwords[listwords[i]] = i\n",
    "\n",
    "# COMPUTING CAPTION PREDICTIONS ON TEST SET\n",
    "predictions = []\n",
    "nbTest = X_test.shape[0]\n",
    "for i in range(0,nbTest,5):\n",
    "    pred = model.predict(X_test[i:i+1,:,:])\n",
    "    wordpreds = []\n",
    "    indpred = np.argmax(pred[0,0,:])\n",
    "    wordpred = listwords[indpred]\n",
    "    wordpreds.append(str(wordpred))\n",
    "    X_test[i,1,100:202] = embeddings[indpred]\n",
    "    cpt=1\n",
    "    while(str(wordpred)!='<end>' and cpt<(X_test.shape[1]-1)):\n",
    "        pred = model.predict(X_test[i:i+1,:,:])\n",
    "        indpred = np.argmax(pred[0,cpt,:])\n",
    "        wordpred = listwords[indpred]\n",
    "        if(wordpred !='<end>'):\n",
    "            wordpreds.append(str(wordpred))\n",
    "            cpt+=1\n",
    "        X_test[i,cpt,100:202] = embeddings[indpred]\n",
    "\n",
    "    if(i%1000==0):\n",
    "        print(\"i=\"+str(i)+\" \"+str(wordpreds))\n",
    "    \n",
    "    predictions.append(wordpreds)\n",
    "\n",
    "# LOADING GROUD TRUTH CAPTIONS ON TEST SET\n",
    "references = []\n",
    "filename = 'flickr_8k_test_dataset.txt'\n",
    "df = pd.read_csv(filename, delimiter='\\t')\n",
    "iter = df.iterrows()\n",
    "\n",
    "ccpt =0\n",
    "for i in range(nbTest//5):\n",
    "    captions_image = []\n",
    "    for j in range(5):\n",
    "        x = iter.__next__()\n",
    "        ll = x[1][1].split()\n",
    "        caption = []\n",
    "        for k in range(1,len(ll)-1):\n",
    "            caption.append(ll[k])\n",
    "\n",
    "    captions_image.append(caption)\n",
    "    ccpt+=1\n",
    "\n",
    "    references.append(captions_image)\n",
    "\n",
    "# COMPUTING BLUE-1, BLUE-2, BLUE-3, BLUE-4\n",
    "blue_scores = np.zeros(4)\n",
    "weights = np.zeros((4,4))\n",
    "weights[0,0] = 1\n",
    "weights[1,0] = 0.5\n",
    "weights[1,1] = 0.5\n",
    "weights[2,0] = 1.0/3.0\n",
    "weights[2,1] = 1.0/3.0\n",
    "weights[2,2] = 1.0/3.0\n",
    "weights[3,:] = 1.0/4.0\n",
    "\n",
    "for i in range(4):\n",
    "    blue_scores[i] = nltk.translate.bleu_score.corpus_bleu(references, predictions, weights = (weights[i,0], weights[i,1], weights[i,2], weights[i,3]) )\n",
    "    print(\"blue_score - \"+str(i)+\"=\"+str( blue_scores[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question :\n",
    "\n",
    "Quelles valeurs de blue scores obtenez-vous ?\n",
    "\n",
    "<a id='dblp-journals-corr-simonyanz14a'></a>\n",
    "\\[SZ15\\] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In *International Conference on Learning Representations (ICLR)*. 2015.\n",
    "\n",
    "<a id='vinyalstbecvpr2015'></a>\n",
    "\\[VTBE15\\] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*. 2015."
   ]
  }
 ],
 "metadata": {
  "date": 1637163670.8748858,
  "filename": "tpVisionLanguage.rst",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "title": "TP 5 - Vision et language"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
